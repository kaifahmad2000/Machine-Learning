{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "Answer: The KNN algorithm is one of the simplest yet highly effective machine learning algorithms. Its strenght lies in its versatality, as it can be used for both classification and regression tasks, although it's more commonly applied in classification.\n",
        "\n",
        "KNN is a versatile supervised learning algorithm that predicts outcomes by analyzing the closest training data points. In classification, it uses majority voting among neighbors, while in regression, it averages their values. Despite being simple and intuitive, it requires careful selection of K and proper feature scaling to perform effectively.\n",
        "\n",
        "Let us take an example to understand how it works:\n",
        "We have to predict the weight of a new data point(let say ID12) based on the given data points:\n",
        "\n",
        "1. The distance between the new point and each training point is calculated.\n",
        "2. The closest k data points are selected(based on the distance).e.g., say closest three data point selected if the value of k is 3.\n",
        "3. The average of these data points is the final prediction for the new point.\n",
        "\n",
        "    For classification, the mode would be the final prediction, but in regression, the average of the weight is taken as the final prediction.\n",
        "To calculate the distance between the data points we can use either euclidean distance or manhattan distance.\n",
        "\n",
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "Answer: Curse of Dimensionality refers to the phenomena where a set of problems arise when data exists in high- dimensional space(i.e., when the number of features/ variables increases). As the dimensionality increases, distance between points become less meaningful, and algorithms that rely on distance or density like KNN face performance issues.\n",
        "Consider a case to better understand it:\n",
        "\n",
        "- Model_1 consists of only two features say the circuit name and the country name.\n",
        "- Model_2 consists of 4 features say weather and max speed of the car including the above two.\n",
        "- Model_3 consists of 8 features say driver's experience, number of wins, car condition, and driver's physical fitness including all the above features.\n",
        "- Model_4 consists of 16 features say driver's age, latitude, longitude, driver's height, hair color, car color, the car company, and driver's marital status including all the above features.\n",
        "- Model_5 consists of 32 features.\n",
        "- Model_6 consists of 64 features.\n",
        "- Model_7 consists of 128 features.\n",
        "- Model_8 consists of 256 features.\n",
        "- Model_9 consists of 512 features.\n",
        "- Model_10 consists of 1024 features.\n",
        "Assuming the training data remains constant, it is observed that on increasing the number of features the accuracy tends to increase until a certain threshold value and after that it remains constant or starts to decrease.\n",
        "  Accuracy:\n",
        "   M1 < M2 < M3 but if we try to extrapolate this trend it doesn't hold true for all the models having more than eight features.\n",
        "\n",
        "    Effect on KNN performance:\n",
        "1. Loss of distance meaning: When distance converge, KNN struggles to distinguish between close and far neighbors.\n",
        "2. High risk of overfitting: With sparse data neighbors are not truly similar, so KNN may overfit to noise.\n",
        "3. Computational Cost: Distance calculations become expensive in high dimensions, slowing the alogrithm.\n",
        "\n",
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "Answer: Principal Component Analysis is a powerful technique used in machine learning for transforming high-dimensional data into a more manageable form. It works by extracting important features known as principal components, which capture the maximum variance in the data. These components are linear combinations of the original features and provide a new coordinate system for the data.\n",
        "Moreover, PCA facilitates distraction-free reading by simplifying complex data while retaining essential information for the analysis. However, it's important to note that PCA assumes linear relationships between variables, which means it may not perform optimally with nonlinear data.\n",
        "Nonetheless, it remains a valuable tool for visualizing data and speeding up algorithms by reducing input dimensions.\n",
        "The steps involved in PCA include:\n",
        "- data standardization,\n",
        "- computation of the covariace matrix, - eigenvalue decomposition,\n",
        "- selection of principle components based on eigenvalues,\n",
        "- projection of data onto these components.\n",
        "\n",
        "PCA serves as a fundamental technique for dimensionality reduction and feature extraction in machine learning.\n",
        "\n",
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Answer: A sqare matrix is associated with special type of vector called an eigenvector. When the matrix acts on the eigenvector. It keeps the direction of the eigenvector unchanged and only scales it by a scaler value called the eigenvalue.\n",
        "    In mathematical terms, for a square matrix A, a non-zero vector v is an eigenvector if:\n",
        "            \n",
        "                A⋅v=λ⋅v\n",
        "where, v is eigenvector\n",
        "       λ is eigenvalue (a scaler value)\n",
        "       A is matrix\n",
        "\n",
        "Imagine you have a matrix A representing a linear transformation, such as stretching, rotating, or scaling a 2D space. When this transformation is applied to a vector v:\n",
        "- Most vectors will change their direction and magnitude.\n",
        "- Some special vectors, however, will only be scaled but not rotated or flipped. These special vectors are eigenvectors like: if lanbda > 1: eigenvector is stretched.\n",
        "if 0 < lambda < 1: the eigenvector is compresseed.\n",
        "if lambda = -1, the eigenvector flips its direction but maintains the same length.\n",
        "\n",
        "Importance of eigenvector and eigenvalues:\n",
        "\n",
        "1. PCA is a widely used technique for dimensionality reduction. Eigenvectors are used to determine the principal components of the data, which capture the maximum variance and help identify the most important features.\n",
        "2. The algorithm that ranks web pages usees eigenvectors of a matrix representing the links between web pages. The principal eigenvector helps determine the relative importance of each page.\n",
        "3. In physics, eigenvectors and eigenvalues describe the state of a system and their measurable properties, such as energy levels.\n",
        "4. Eigenvector are used in facial recognition systems, particularly techniques like Eigenfaces, where they help represent images as linear combination of singnificant features.\n",
        "5. In engineering, eigenvectors describe the modes of vibration in structures like bridges and buildings.\n",
        "\n",
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "Answer: PCA compliments KNN in a pipeline by first performing dimensionality reduction, which helps to overcome the curse of dimensionality and improves the efficiency and accuracy of KNN by transforming high dimensional data into a lower dimensional space.\n",
        "1. Dimensionality Reduction with PCA:\n",
        "- Noise Reduction: PCA identifies the principle components that capture the most variance in the data, effectively reducing noise that might be present in the original high-dimensional features.\n",
        "- Feature Extraction: PCA transforms the original features into a new set of uncorrelated principal components. These components represent the most significant underlying patterns in the data.\n",
        "- Reduced Dimensionality: By selecting a subset of the most important principal components, PCA projects the data into a lower-dimensional space, which is more computationally tractable.\n",
        "\n",
        "2. Improved KNN Performance:\n",
        "- Faster Computation: With fewer features to consider in the lower dimensional space, the distance calculations performed by KNN become significantly faster, reducing the algorithm's overall computational complexity.\n",
        "- Enhanced Accuracy: By removing irrelevant and redundant features and noise, PCA can make the data patterns more discernible.This leads to better seperation between classes in the reduced feature space, allowing KNN to make more accurate predictions.\n",
        "- Avoidance of the Curse of Dimensionality: In high dimensional spaces, KNN can struggle because the concept of 'nearness' becomes less meaningful as data points become sparse. PCA mitigate this by creating a more informative, lower dimensional representation.\n",
        "\n"
      ],
      "metadata": {
        "id": "MshRJX5BbnUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report, mean_absolute_error\n"
      ],
      "metadata": {
        "id": "QNsnlEEowF2j"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "#scaling. Compare model accuracy in both cases.\n",
        "\n",
        "# data preprocessing\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "# model_training without scaling\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scale)\n",
        "\n",
        "# model_training with scaling\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "clf.fit(x_train, y_train)\n",
        "Y_pred = clf.predict(x_test)\n",
        "accuracy_with_scale = accuracy_score(y_test, Y_pred)\n",
        "\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scale)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0HNIDoHxYf3",
        "outputId": "08804e42-fa0d-4806-9554-d224d744f443"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7037037037037037\n",
            "Accuracy with scaling: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "# ratio of each principal component.\n",
        "#data preprocessing\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, random_state = 1)\n",
        "\n",
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# PCA\n",
        "pca = PCA()\n",
        "pca.fit_transform(x_train)\n",
        "pca.transform(x_test)\n",
        "\n",
        "exp_variance = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(exp_variance):\n",
        "  print(f\"Principal Component {i+1}: {ratio}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhbCRNvr7Y8e",
        "outputId": "7c447b2b-7205-4550-ec3f-33b2d1de00fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal Component 1: 0.35168280854725753\n",
            "Principal Component 2: 0.19739102667462324\n",
            "Principal Component 3: 0.11318948926631382\n",
            "Principal Component 4: 0.07729222031371188\n",
            "Principal Component 5: 0.06125163621403974\n",
            "Principal Component 6: 0.05129145201022682\n",
            "Principal Component 7: 0.04229865732302875\n",
            "Principal Component 8: 0.026249245002008818\n",
            "Principal Component 9: 0.02426133805913909\n",
            "Principal Component 10: 0.018242677233753037\n",
            "Principal Component 11: 0.015803322570635214\n",
            "Principal Component 12: 0.013243352038909289\n",
            "Principal Component 13: 0.007802774746352733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "# components). Compare the accuracy with the original dataset.\n",
        "\n",
        "#data preprocessing\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "#scaling\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# apply PCA\n",
        "pca = PCA(n_components = 2)\n",
        "x_trained_PCA = pca.fit_transform(x_train)\n",
        "x_test_PCA = pca.transform(x_test)\n",
        "\n",
        "# model_training without PCA\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "accuracy_no_PCA = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#model_training with PCA\n",
        "clf.fit(x_trained_PCA, y_train)\n",
        "y_pred_PCA = clf.predict(x_test_PCA)\n",
        "accuracy_with_PCA = accuracy_score(y_test, y_pred_PCA)\n",
        "\n",
        "# comparing values\n",
        "print(\"Accuracy of the model with original dataset:\", accuracy_no_PCA)\n",
        "print(f\"Accuracy of model after applying PCA: {accuracy_with_PCA}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M0nPFcJMFI3",
        "outputId": "a5c3acdb-5ae8-49a3-adab-8938dbe726db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model with original dataset: 0.9814814814814815\n",
            "Accuracy of model after applying PCA: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "# manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "# data preprocessing\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "#train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "# scaling\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# define_model_with_metrics\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors = 5, metric = 'manhattan')\n",
        "\n",
        "# model_training and evaluation\n",
        "knn_euclidean.fit(x_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(x_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "knn_manhattan.fit(x_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(x_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# comparing accuracy\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "romInL-NMI4e",
        "outputId": "7f2d3a3a-d67d-41bf-932f-1148978c12f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9814814814814815\n",
            "Accuracy with Manhattan distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data.\n",
        "\n",
        "Answer: Due to large number of features and a small number of samples, traditional models overfit this can happen due to a phenomenon called curse of dimensionality, to remove this we can use PCA.\n",
        "\n",
        "Step 1:- Using PCA for dimensionality reduction: Biomedical datasets often contain hundreds or thousands of features (e.g., gene expressions, imaging biomarkers, lab test results). Many of these features are correlated or redundant, which increases noise and makes models less efficient.\n",
        "\n",
        "- Principal Component Analysis transforms the original features into new uncorrelated features called principal components, ordered by the amount of variance they explain.\n",
        "\n",
        "- Before applying PCA, we standardize features (zero mean, unit variance), since PCA is sensitive to scale.\n",
        "\n",
        "- PCA reduces data complexity while preserving most of the information.\n",
        "\n",
        "    This step improves computational efficency and reduces the risk of overfitting.\n",
        "\n",
        "Step 2:- Deciding the number of components:\n",
        "- Explained Variance Ratio: We calculate how much variance each principal component explains.\n",
        "\n",
        "- Cumulative Variance Plot (Scree Plot): Used to find the \"elbow point,\" where adding more components doesn't significantly increase variance explained.\n",
        "\n",
        "- Typical Criterion: Keep enough components to retain 90 to 95% variance.\n",
        "\n",
        "Example: If the first 10 components explain 94% of the variance, we use those instead of all 100 original features.\n",
        "\n",
        "  This ensures that we had balanced between dimensionality reduction and retaining meaningful biomedical signals.\n",
        "\n",
        "Step 3:- Apply KNN for classification:\n",
        "- Once PCA reduced the dataset, we train a K-Nearest Neighbors classifier on the principal components.\n",
        "- Post PCA, Each new patient is classified based on the \"closest\" existing patient profiles in the reduced feature space.\n",
        "- PCA removes irrelevant noise, making these distances more meaningful.\n",
        "\n",
        "  With fewer dimensions, KNN avoids the curse of dimensionality and performs more reliability.\n",
        "\n",
        "Step 4:- Model Evaluation: For evaluation purposes we can use:\n",
        "- Cross Validation: Use Grid Search CV or Randomized Search CV for reliable estimates.\n",
        "- could use metrics to know the performace of the model-\n",
        "  - Accuracy for overall correctness.\n",
        "  - Precision and Recall are critical in medical tasks i.e., high recall means fewer missed patients with disease.\n",
        "  - F1-score can make balances between precision and recall.\n",
        "  - ROC-AUC measures the ability to distinguish diseased and healthy patients.\n",
        "\n",
        "  could show the performance of KNN with and without PCA, demonstrate the benefit of dimensionality reduction.\n",
        "Step 5:- Justification:\n",
        "- Biomedical data is large, complex, and noisy. PCA helps us compress the data into fewer meaningful patterns.\n",
        "- This makes predictions faster, more accurate, and less likely to overfit.\n",
        "- KNN is intuitive: the model predicts based on the closest patient profiles, which is easy for medical experts to understand.\n",
        "- By evaluating with recall and ROC-AUC, we ensure the model prioritizes catching as many disease cases as possible, even if it slightly lowers accuracy.\n",
        "\n",
        "Business/Healthcare Value:\n",
        "- Helps doctors with early disease detection.\n",
        "- Enables personalized treatments by grouping similar patients.\n",
        "- Improves trust and interpretability, since PCA can show which biomedical signals are most influential.\n",
        "- Reduces computational cost → scalable to hospital-level data."
      ],
      "metadata": {
        "id": "PPn3jM1ATeeT"
      }
    }
  ]
}