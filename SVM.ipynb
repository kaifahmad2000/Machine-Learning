{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Answer: A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "Its main goal is to find the best boundary (hyperplane) that separates different classes in the feature space.\n",
        "SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane. Following this, characteristics of new data can be used to predict the group to which a new record should belong.For example: Imagine a dataset in which the data points fall into two different categories. The two categories can be seperated with a curve. After the transformation, the boundary between the two categories can be defined by a hyperplane. The mathematical function used for the transformation is known as the kernel function. SVM supports linear, polynomial, radial bias function, sigmoid function kernel types.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "Answer: Suppose we have data points that we want to classify into two groups. When the data is linearly separable, and we don't want to have any misclassifications that's why we use SVM with a hard margin. However, when a linear boundary is not feasible, or we want to allow some misclassifications in the hope of achieving better generality, we can opt for a soft margin for our classifier.\n",
        "1. SVM with Hard Margin: In a hard margin SVM, the objective is to identify a hyperplane that completely separates data points belonging to different classes, ensuring a clear demarcation with the utmost margin width possible. This margin is the distance between the hyperplane and the nearest data point, also known as the support vectors.\n",
        "The hyperplane equation plays a crucial role in hard margin SVMs because it defines the boundary that separates the classes. Ideally, we want this boundary to have a maximum margin from the nearest data points of each class. The objective function in hard margin SVM aims to find the weight vector and bias term that maximize this margin while ensuring that all data points are correctly classified. The decision boundary is solely determined by these support vectors, and any data points falling on the wrong side of the hyperplane contribute to the margin violation. Mathematically, for a linearly seperable dataset, the decision function of a hard margin SVM can be expressed as:\n",
        "        yi(wTx + b) > 1\n",
        "  where, x is the weight vector perpendicular to the plane.\n",
        "  w is the weight of feature vector\n",
        "  b is the bias term\n",
        "The decision boundary is determined by the hyperplane equation\n",
        "          wTx + b = 0\n",
        "This equation essentially states that any data point lying on the hyperplane satisfies this equation. In other words, the dot product of the feature vector and the weight vector, shifted by the bias term, equals zero for points on the decision boundary.\n",
        "      margin = 1/ |w|\n",
        "\n",
        "2. SVM with a Soft Margin: Soft Margin SVM introduces flexibility by allowing some margin violations (misclassifications) to handle cases where the data is not perfectly separable. Suitable for scenarios where the data may contain noise or outliers. It Introduces a penalty term for misclassifications, allowing for a trade-off between a wider margin and a few misclassifications.\n",
        "\n",
        "Soft margin SVM allows for some margin violations, meaning that it permits certain data points to fall within the margin or even on the wrong side of the decision boundary. This adaptability is managed by a factor called C, also called the \"regularization parameter,\" which helps find a balance between making the gap as big as possible and reducing mistakes in grouping things.\n",
        "Mathematically, decision function of a soft margin SVM can be defined as:\n",
        "    yi(wTxi + b) > 1 - margin violation\n",
        "  where, yi is the target variable\n",
        "\n",
        "1/2|w|2 + C summation of Ni*margin violation\n",
        "\n",
        "The equation combines the objectives of maximizing the margin (represented by the first term) and minimizing the penalty for margin violations (represented by the second term). The regularization parameter (C) controls the trade-off between these objectives. A higher C value prioritizes a wider margin, even if it allows some misclassifications. Conversely, a lower C value allows for more margin violations to achieve a smoother\n",
        "decision boundary.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Answer:The kernel trick is a method used in SVMs to enable them to classify non-linear data using a linear classifier. By applying a kernel function, SVMs can implicitly map input data into a higher-dimensional space where a linear separator (hyperplane) can be used to divide the classes. This mapping is computationally efficient because it avoids the direct calculation of the coordinates in this higher space.\n",
        "Radial Bias Function: The RBF kernel is the most widely used kernel in SVM. It creates non linear combination of features to bring your feature in higher dimension. The formula of RBF kernel is:\n",
        "f(x1,x2) = e^-(||x1-x2||/ 2sigma^2)\n",
        "where, ||x1-x2|| is euclidian distance between two point x1 and x2.\n",
        "sigma  is variance/ hyperparameter.\n",
        "Use case: RBF is used whenever the data in the radial format or curve format, can segregate using RBF function.\n",
        "\n",
        "Question 4: What is a Naive Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Answer: Naive Bayes is a classification algorithm that uses probability to predict which category a data point belongs to, assuming that all features are independent.The main idea behind the Naive Bayes classifier is to use Bayes' Theorem to classify data based on the probabilities of different classes given the features of the data. It is used mostly in high-dimensional text classification\n",
        "- The Naive Bayes Classifier is a simple probabilistic classifier and it has very few number of parameters which are used to build the ML models that can predict at a faster speed than other classification algorithms.\n",
        "- It is a probabilistic classifier because it assumes that one feature in the model is independent of existence of another feature. In other words, each feature contributes to the predictions with no relation between each other.\n",
        "- Naïve Bayes Algorithm is used in spam filtration, Sentimental analysis, classifying articles and many more.\n",
        "\n",
        "It is named as \"Naive\" because it assumees the presence of one feature doesnot affect other features.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "Answer:\n",
        "Gaussian Naive Bayes: In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values.\n",
        "\n",
        "Multinomial Naive Bayes: It is used when features represent the frequency of terms (such as word counts) in a document. It is commonly applied in text classification, where term frequencies are important.\n",
        "\n",
        "Bernoulli Naive Bayes: It deals with binary features, where each feature indicates whether a word appears or not in a document. It is suited for scenarios where the presence or absence of terms is more relevant than their frequency. Both models are widely used in document classification tasks.\n",
        "\n",
        "Multinomial and Bernoulli Naive Bayes is used for text data in NLP.\n",
        "\n",
        "Question 10: Imagine you're working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "- Text with diverse vocabulary\n",
        "- Potential class imbalance (far more legitimate emails than spam)\n",
        "- Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "- Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "- Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "- Address class imbalance\n",
        "- Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "Answer:\n",
        "Data Preprocessing: Under this step you can handle missing data like dropping the rows which contains missing emails, clean the text like: convert it into lower case, remove punctuations,HTML tags, and special characters,and extract the features with the help of TFIDF which is called vectorization.\n",
        "\n",
        "Choosing a model: We can choose either Naive Bayes or Support Vector Machine.\n",
        "Use naive bayes for classifying text data, it assumes that the features are independent and performs well on spam detection.\n",
        "SVM is used to handles high dimensional text data. Its only demerit is it is computationally heavier but can achieve higher accuracy than naive bayes.\n",
        "\n",
        "Handling class imbalance: To handle class imbalance we can use resampling techniques like: SMOTE or remove some non-spam emails, class weights(class_weight = 'balanced'), or threshold tuning like: adjusting the decision threshold to favour spam recall. We can use any one of the following technique to handle class imbalance.\n",
        "\n",
        "Evaluation Metrics: For evaluating the performance of the model, use precision, recall, f1-score, roc-auc, confusion matrix from the sklearn.metrics module.\n",
        "\n",
        "In building the model we should keep in mind that a false negative (missing the spam email) cannot harm much to the user than false positive(legit emails marked as spam).\n",
        "\n",
        "Business Impact:\n",
        "1. Employees spend less time filtering spam which can increase their productivity.\n",
        "2. Users get fewer spam emails in their inbox leads to satisfaction of the customer.\n",
        "3. Catching/ Phishing emails reduce the risk of data beaches.\n",
        "4. Automated filtering reduces manual monitoring efforts.\n"
      ],
      "metadata": {
        "id": "IPno5ftMcHGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Xx3WrwCzAAZa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris dataset\n",
        "#● Train an SVM Classifier with a linear kernel\n",
        "#● Print the model's accuracy and support vectors.\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 1)\n",
        "clf = SVC(kernel = 'linear')\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(f\"The model accuracy is: {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"The support vectors are: {clf.support_vectors_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN8dmgHodZpG",
        "outputId": "cf0c4b46-9021-41fb-ae8a-8168ba5a5fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is: 1.0\n",
            "The support vectors are: [[5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.  3.  4.8 1.8]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [6.3 2.8 5.1 1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset\n",
        "#● Train a Gaussian Naïve Bayes model\n",
        "#● Print its classification report including precision, recall, and F1-score.\n",
        "cancer = load_breast_cancer()\n",
        "x, y = cancer.data, cancer.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "YmbibkJWB5Im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba82128-2728-49dd-b016-6321531f2c6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93        63\n",
            "           1       0.95      0.96      0.96       108\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.94      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "#C and gamma.\n",
        "#● Print the best hyperparameters and accuracy.\n",
        "wine = load_wine()\n",
        "x, y  = wine.data, wine.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "param = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'kernel': ['rbf', 'polynomial']\n",
        "}\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "grid = GridSearchCV(svc, param_grid = param, scoring = 'accuracy', cv = 5, verbose = 1)\n",
        "grid.fit(x_train, y_train)\n",
        "print(f\"The best hyperparameters are: {grid.best_params_}\")\n",
        "print(f\"The best accuracy is: {grid.best_score_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYC3ilUwHsay",
        "outputId": "bc2d2f6c-d7b8-4f21-d904-938f63118f6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
            "The best hyperparameters are: {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "The best accuracy is: 0.8463333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "#sklearn.datasets.fetch_20newsgroups).\n",
        "#● Print the model's ROC-AUC score for its predictions.\n",
        "#data ingestion/ preprocessing\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups(subset = 'all', categories = None)\n",
        "categories = fetch_20newsgroups(subset = 'all').target_names\n",
        "print(\"All categories in the dataset:\")\n",
        "for cat in categories:\n",
        "    print(cat)\n",
        "categorize = ['rec.motorcycles', 'sci.electronics', 'comp.graphics', 'talk.politics.mideast']\n",
        "newsgroup = fetch_20newsgroups(subset = 'all', categories = categorize)\n",
        "X, y = newsgroup.data, newsgroup.target\n",
        "\n",
        "# convert text to numeric\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n",
        "x_dense = X.toarray()\n",
        "\n",
        "#train-test-split\n",
        "x_train,x_test, y_train, y_test = train_test_split(x_dense, y, test_size = 0.25, random_state = 1)\n",
        "\n",
        "#model training\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train, y_train)\n",
        "y_prob = gnb.predict_proba(x_test)\n",
        "\n",
        "# roc-auc score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc = roc_auc_score(y_test, y_prob, multi_class = 'ovr')\n",
        "print(f\"ROC-AUC Score : {roc_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD6vSmAWJij7",
        "outputId": "887d3cc5-a381-473c-eee0-183082f1185d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All categories in the dataset:\n",
            "alt.atheism\n",
            "comp.graphics\n",
            "comp.os.ms-windows.misc\n",
            "comp.sys.ibm.pc.hardware\n",
            "comp.sys.mac.hardware\n",
            "comp.windows.x\n",
            "misc.forsale\n",
            "rec.autos\n",
            "rec.motorcycles\n",
            "rec.sport.baseball\n",
            "rec.sport.hockey\n",
            "sci.crypt\n",
            "sci.electronics\n",
            "sci.med\n",
            "sci.space\n",
            "soc.religion.christian\n",
            "talk.politics.guns\n",
            "talk.politics.mideast\n",
            "talk.politics.misc\n",
            "talk.religion.misc\n",
            "ROC-AUC Score : 0.9691991749402641\n"
          ]
        }
      ]
    }
  ]
}