{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Answer: Ensemble Learning is a machine learning models by combining predictions from multiple models. By leveraging the strengths of diverse algorithms, ensemble methods aim to reduce both bais and variance, resulting in more reliable predictions. It also increases the model's robustness to errors and uncertainities, especially in critical applications like: healthcare and finance.\n",
        "In simple words, we can say that ensemble learning means combining multiple models or algorithms that can improve prediction's accuracy.\n",
        "\n",
        "Ensemble learning techniques like: bagging, boosting and stacking enhance performance and reliability making them valuable for teams that want to build reliable ML systems.\n",
        "The core idea behind ensemble learning is that a single model may be biased or high variance, so if we combine multiple models, the accuracy of this model is better than the individual models.\n",
        "It works by training multiple models and combining there output by using:\n",
        "- average for regression and voting for classification datasets.\n",
        "- using bagging, boosting and stacking techniques.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer: Bagging is a homogeneous weak learners' model that learns parallely from each other and combines them for determining the model average.\n",
        "Boosting is also homogeneous weak learners' model but in this learners' learn model sequentially and adaptively to improve model predictions of a learning algorithm.\n",
        "\n",
        "-  Differences:\n",
        "1. The main goal of bagging is to reduce variance and prevent overfitting whereas boosting reduce bias and make weak learners strong.\n",
        "\n",
        "2. Bagging trains multiple models independently in parallel on different sample subsets of data while boosting models trained squentially, each new model focuses on correcting errors of the previous one.\n",
        "\n",
        "3. Bagging uses bootstrapping(sampling while replacement) while Boosting uses entire dataset, but data points are re-weighted for data sampling.\n",
        "\n",
        "4. Prediction combined by majority vote/ average for classification/ regression while predictions combined using a weighted vote/ weighted sum.\n",
        "\n",
        "5. Bagging is less prone to overfitting as compare to boosting.\n",
        "\n",
        "6. Bagging uses Random Forest algorithm while Boosting uses AdaBoost, Gradient Boost, XGBoost, CatBoost etc.\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer: Bootstrap sampling is a method that involves drawing of sample data repeatedly with replacement from a data source to estimate a population parameter.\n",
        "Analogy or basic idea of bootstrap sampling: Suppose we have to take the average height of students in a school. First method is that we calcuate the height of each student but it will a tedious task rather than this we can take 5 samples for 20 times and takes the average of these 100 students. This method is basic idea of bootstrap sampling.\n",
        "Bootstrap sampling is used in a machine learning ensemble algorithm called bootstrap aggregating(also called as bagging). It helps in avoiding overfitting and improves the stability of machine learning algorithms.\n",
        "\n",
        "Bootstrap sampling in bagging can diversify the models, reduces variance, out-of-bag(OOB) error estimation, avoids overfitting.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer: OOB score is a very powerful validation technique used especially for the Random Forest algorithm for least Variance results.\n",
        "While using the cross validation technique, every validation set has already been seen or used in training by a few decision trees and hence there is a leakage of data, therefore more variance. But, OOB score prevents leakage and gives a better model with low variance.\n",
        "The OOB score is computed as the number of correctly predicted rows from the out-of-bag sample.\n",
        "OOB score used to evaluate ensemble models:\n",
        "\n",
        "1. Model Evaluation without a test set:\n",
        "For each tree, predictions are made only on its OOB samples. This provides an unbiased performance estimate because the tree is evaluated on unseen data.\n",
        "\n",
        "2. OOB score calculation: For classificaiton majority vote is taken across all trees while for regression average of all the individual trees are taken out.\n",
        "\n",
        "3. Compare the predictions with actual labels: Compute accuracy for classification or R2/ mean squared for regression.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs a\n",
        "Random Forest.\n",
        "\n",
        "Answer: Feature Importance in a single Decision Tree:\n",
        "- A Decision Tree decides feature importance based on how much a feature reduces impurity (e.g., Gini index or entropy for classification, variance for regression).\n",
        "\n",
        "- At each split, it calculates the information gain (or reduction in impurity).\n",
        "\n",
        "- The importance of a feature is the sum of all impurity reductions that the feature contributes across the entire tree.\n",
        "\n",
        "- These importances are then normalized so they add up to 1.\n",
        "\n",
        "Feature importance in a Random Forest:\n",
        "- Random Forests aggregate feature importance across many trees.\n",
        "- For each tree:\n",
        "\n",
        "- Compute feature importance as in a single tree (impurity reduction).Then:Average the importances over all trees.\n",
        "\n",
        "Feature selection in a Random Forest is more robust and stable, handles correlation better and reflects the true predictive power across unseen data.\n"
      ],
      "metadata": {
        "id": "D7sRuFE0Po2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer, load_iris, fetch_california_housing\n",
        "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, r2_score, mean_squared_error"
      ],
      "metadata": {
        "id": "8ONoJjEPHEDJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.'''\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)\n",
        "\n",
        "#model_training\n",
        "rf = RandomForestClassifier(n_estimators = 100, random_state = 4, verbose = 2)\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "# model prediction and evaluation\n",
        "feature_importance = pd.Series(rf.feature_importances_, index = data.feature_names).sort_values(ascending = False)\n",
        "print(f\"Most importance features are: \\n{feature_importance[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6UDJtSkpUQp",
        "outputId": "c386f1b7-2ee1-4b8b-9909-dc5b168c6fe1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 1 of 100\n",
            "building tree 2 of 100\n",
            "building tree 3 of 100\n",
            "building tree 4 of 100\n",
            "building tree 5 of 100\n",
            "building tree 6 of 100\n",
            "building tree 7 of 100\n",
            "building tree 8 of 100\n",
            "building tree 9 of 100\n",
            "building tree 10 of 100\n",
            "building tree 11 of 100\n",
            "building tree 12 of 100\n",
            "building tree 13 of 100\n",
            "building tree 14 of 100\n",
            "building tree 15 of 100\n",
            "building tree 16 of 100\n",
            "building tree 17 of 100\n",
            "building tree 18 of 100\n",
            "building tree 19 of 100\n",
            "building tree 20 of 100\n",
            "building tree 21 of 100\n",
            "building tree 22 of 100\n",
            "building tree 23 of 100\n",
            "building tree 24 of 100\n",
            "building tree 25 of 100\n",
            "building tree 26 of 100\n",
            "building tree 27 of 100\n",
            "building tree 28 of 100\n",
            "building tree 29 of 100\n",
            "building tree 30 of 100\n",
            "building tree 31 of 100\n",
            "building tree 32 of 100\n",
            "building tree 33 of 100\n",
            "building tree 34 of 100\n",
            "building tree 35 of 100\n",
            "building tree 36 of 100\n",
            "building tree 37 of 100\n",
            "building tree 38 of 100\n",
            "building tree 39 of 100\n",
            "building tree 40 of 100\n",
            "building tree 41 of 100\n",
            "building tree 42 of 100\n",
            "building tree 43 of 100\n",
            "building tree 44 of 100\n",
            "building tree 45 of 100\n",
            "building tree 46 of 100\n",
            "building tree 47 of 100\n",
            "building tree 48 of 100\n",
            "building tree 49 of 100\n",
            "building tree 50 of 100\n",
            "building tree 51 of 100\n",
            "building tree 52 of 100\n",
            "building tree 53 of 100\n",
            "building tree 54 of 100\n",
            "building tree 55 of 100\n",
            "building tree 56 of 100\n",
            "building tree 57 of 100\n",
            "building tree 58 of 100\n",
            "building tree 59 of 100\n",
            "building tree 60 of 100\n",
            "building tree 61 of 100\n",
            "building tree 62 of 100\n",
            "building tree 63 of 100\n",
            "building tree 64 of 100\n",
            "building tree 65 of 100\n",
            "building tree 66 of 100\n",
            "building tree 67 of 100\n",
            "building tree 68 of 100\n",
            "building tree 69 of 100\n",
            "building tree 70 of 100\n",
            "building tree 71 of 100\n",
            "building tree 72 of 100\n",
            "building tree 73 of 100\n",
            "building tree 74 of 100\n",
            "building tree 75 of 100\n",
            "building tree 76 of 100\n",
            "building tree 77 of 100\n",
            "building tree 78 of 100\n",
            "building tree 79 of 100\n",
            "building tree 80 of 100\n",
            "building tree 81 of 100\n",
            "building tree 82 of 100\n",
            "building tree 83 of 100\n",
            "building tree 84 of 100\n",
            "building tree 85 of 100\n",
            "building tree 86 of 100\n",
            "building tree 87 of 100\n",
            "building tree 88 of 100\n",
            "building tree 89 of 100\n",
            "building tree 90 of 100\n",
            "building tree 91 of 100\n",
            "building tree 92 of 100\n",
            "building tree 93 of 100\n",
            "building tree 94 of 100\n",
            "building tree 95 of 100\n",
            "building tree 96 of 100\n",
            "building tree 97 of 100\n",
            "building tree 98 of 100\n",
            "building tree 99 of 100\n",
            "building tree 100 of 100\n",
            "Most importance features are: \n",
            "worst concave points    0.166809\n",
            "worst perimeter         0.154904\n",
            "mean concave points     0.131314\n",
            "worst radius            0.121564\n",
            "worst area              0.071830\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    0.1s\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree'''\n",
        "data1 = load_iris()\n",
        "X = data1.data\n",
        "y = data1.target\n",
        "\n",
        "#train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "#model_training\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "dt_clf.fit(x_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(x_test)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "#train model using bagging\n",
        "bagging_clf = BaggingClassifier(estimator = dt_clf,\n",
        "                                n_estimators = 50,\n",
        "                                random_state = 10)\n",
        "bagging_clf.fit(x_train, y_train)\n",
        "y_pred_bag = bagging_clf.predict(x_test)\n",
        "bag_acc = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "#comapare the accuracy\n",
        "print(f\"Single Decision Tree Accuracy: {dt_acc}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bag_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cimrgpXjILzQ",
        "outputId": "add501c6-5b68-4783-ba40-992b3b53a6fb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 0.9555555555555556\n",
            "Bagging Classifier Accuracy: 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy'''\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "#train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
        "\n",
        "#model_training_and_hyperparameter tuning\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\": [50, 10, 100, 200],\n",
        "    \"max_depth\": [None, 1, 5, 10, 20]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = rf_clf,\n",
        "    param_grid = params,\n",
        "    cv = 5,\n",
        "    scoring = 'accuracy',\n",
        "    n_jobs = 1\n",
        ")\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Accuracy: {best_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXn8ZnrxT4uy",
        "outputId": "69bd586f-c2f0-46c0-c41b-eb62abd1d996"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 10}\n",
            "Best Accuracy: 0.9717920656634746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)'''\n",
        "data2 = fetch_california_housing()\n",
        "df = pd.DataFrame(data2.data, columns = data2.feature_names)\n",
        "df['target'] = data2.target\n",
        "x = df.drop('target', axis = 1)\n",
        "y = df['target']\n",
        "\n",
        "#train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n",
        "\n",
        "#model training\n",
        "models = {\n",
        "    \"DT Regressor\": DecisionTreeRegressor(),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(),\n",
        "    \"Multiple Linear Regression\": LinearRegression(),\n",
        "    \"Bagging Regressor\": BaggingRegressor()\n",
        "}\n",
        "\n",
        "#multi_model_training_eval\n",
        "def model_train_eval(x_train, x_test, y_train, y_test, models):\n",
        "    evaluation = {}\n",
        "    for i in range(len(models)):\n",
        "      model = list(models.values())[i]\n",
        "      model.fit(x_train, y_train)\n",
        "      y_pred = model.predict(x_test)\n",
        "      model_score = mean_squared_error(y_test, y_pred)\n",
        "      evaluation[list(models.keys())[i]] = model_score\n",
        "    return evaluation\n",
        "\n",
        "model_train_eval(x_train, x_test, y_train, y_test, models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJzUlob9W_Q0",
        "outputId": "fd2ff8b0-9f51-4684-fa3a-5a58d8e42415"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DT Regressor': 0.5052509005046996,\n",
              " 'Random Forest Regressor': 0.260355466532268,\n",
              " 'Multiple Linear Regression': 0.5296293151408235,\n",
              " 'Bagging Regressor': 0.28996822164540553}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer: To predict if a customer will default on loan or not we have to first understand the dataset.\n",
        "The dataset must include all the necessary information required for the prediction like: Demographic detais and\n",
        "transaction history such as: loan amount, repayment history, balances etc). As the dataset contains imbalanced\n",
        "classes and high dimensionality, we have to acknowledge that while model building.\n",
        "After these simple steps. Now, we can choose between Bagging or Boosting.\n",
        "Bagging:\n",
        "  It reduces variance and works well if the overfitting is an issue but it's only limitation is that it may not capture rare default cases strongly.\n",
        "Boosting:\n",
        "  Boosting focuses more on misclassified cases and generally performs bettter on imbalanced classification problems.If not tuned properly can cause overfitting.\n",
        "\n",
        "In general, for predicting loan default cases boosting algorithm like XGBOOST is preferable.\n",
        "\n",
        "2. Handling Overfitting: In bagging, to reduce overfitting we can limit the depth(max_depth), use more trees(n_estimators), and randomly select the features(max_features)\n",
        "while in boosting, to reduce overfitting, we can control learning rate, regularization, and early stopping to prevent unnecessary rounds.\n",
        "\n",
        "3. Select Base Model: use decision trees, for bagging, create multiple independent trees and for boosting use sequential trees, each correcting previous errors.\n",
        "\n",
        "4. Cross Validaton and performance evaluation: For cross validation, use Randomized SearchCV or Grid SearchCV and for model evaluation use\n",
        "ROC-AUC curve, confusion matrix, and classfication report\n",
        "\n",
        "5. Business Impact: - Reduces financial risk as using the model very few bad loans got approved.\n",
        "- Data driven loan decision making can improve the customer experience as it will make the process faster.\n",
        "- Feature importance analysis ensures transparancy.\n",
        "- Make the lender's account cleaner, increase their profit and minizee the risk of deafault.\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "x5wu9pvbbszk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPZ5CYnHhbh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}